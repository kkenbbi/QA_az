{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "054b36c6",
      "metadata": {
        "id": "054b36c6"
      },
      "outputs": [],
      "source": [
        "# pip install -q datasets transformers sentence-transformers scikit-learn faiss-cpu nltk rouge_score evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48bd036b",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "801cd2cf794f42a5933dafba22a07b73"
          ]
        },
        "id": "48bd036b",
        "outputId": "70d1ef38-e071-4c22-a38c-ed1124cbfd33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 0. Setup & Data Loading/Preprocessing ---\n",
            "Loading eqanun dataset...\n",
            "Dataset loaded. Number of documents: 50989\n",
            "Dataset features: {'text': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None)}\n",
            "Example document: {'text': 'Azərbaycan Respublikasının İnzibati Xətalar Məcəlləsində dəyişiklik edilməsi haqqında\\nAZƏRBAYCAN RESPUBLİKASININ QANUNU\\nAzərbaycan Respublikasının Milli Məclisi Azərbaycan Respublikası Konstitusiyasının 94-cü maddəsinin I hissəsinin 17-ci bəndini rəhbər tutaraq\\nqərara alır:\\nMaddə 1.\\nAzərbaycan Respublikasının İnzibati Xətalar Məcəlləsinin\\n(Azərbaycan Respublikasının Qanunvericilik Toplusu, 2016, № 2 (I kitab), maddə 202, № 3, maddələr 397, 403, 429, № 4, maddələr 631, 647, 654, № 5, maddələr 835, 846, № 6, maddələr 997, 1010, № 7, maddələr 1247, 1249, № 10, maddə 1608, № 11, maddələr 1769, 1774, 1781, 1783, 1786, 1788, № 12, maddələr 1984, 2000, 2009, 2024, 2049; 2017, № 1, maddə 21, № 2, maddələr 139, 147, 152, 162, № 3, maddələr 331, 344, № 5, maddələr 698, 701, 734, 749, 754, № 6, maddələr 1020, 1033, 1036, № 7, maddələr 1273, 1296, 1297, 1299; Azərbaycan Respublikasının 2017-ci il 20 oktyabr tarixli 814-VQD və 817-VQD nömrəli qanunları) 566-cı maddəsinin adında və dispozisiyasında “Şəxsiyyət vəsiqəsinin, pasportun, dənizçinin” sözləri “Dənizçinin” sözü ilə əvəz edilsin.\\nMaddə 2.\\nBu Qanun 2018-ci il yanvarın 1-dən qüvvəyə minir.\\nİlham ƏLİYEV,\\nAzərbaycan Respublikasının Prezidenti\\nBakı şəhəri, 1 dekabr 2017-ci il\\n№ 913-VQD', 'id': '37260'}\n",
            "Chunking documents...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "801cd2cf794f42a5933dafba22a07b73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50989 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1575 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total documents: 50989. Total chunks created: 285300\n",
            "Example chunk: {'doc_id': '37260_0', 'original_doc_id': '37260', 'text': 'Azərbaycan Respublikasının İnzibati Xətalar Məcəlləsində dəyişiklik edilməsi haqqında AZƏRBAYCAN RESPUBLİKASININ QANUNU Azərbaycan Respublikasının Milli Məclisi Azərbaycan Respublikası Konstitusiyasının 94 - cü maddəsinin I hissəsinin 17 - ci bəndini rəhbər tutaraq qərara alır : Maddə 1. Azərbaycan Respublikasının İnzibati Xətalar Məcəlləsinin ( Azərbaycan Respublikasının Qanunvericilik Toplusu, 2016, № 2 ( I kitab ), maddə 202, № 3, maddələr 397, 403, 429, № 4, maddələr 631, 647, 654, № 5, maddələr 835, 846, № 6, maddələr 997, 1010, № 7, maddələr 1247, 1249, № 10, maddə 1608, № 11, maddələr 1769, 1774, 1781, 1783, 1786, 1788, № 12, maddələr 1984, 2000, 2009, 2024, 2049 ; 2017, № 1, maddə 21, № 2, maddələr 139, 147, 152, 162, № 3, maddələr 331, 344, № 5, maddələr 698, 701, 734, 749, 754, № 6, maddələr 1020, 1033, 1036, № 7, maddələr 1273, 1296, 1297, 1299 ; Azərbaycan Respublikasının 2017 - ci il 20 oktyabr tarixli 814 - VQD və 817 - VQD nömrəli qanunları ) 566 - cı maddəsinin adında və dispozisiyasında [UNK] Şəxsiyyət vəsiqəsinin, pasportun, dənizçinin [UNK] sözləri [UNK] Dənizçinin [UNK] sözü ilə əvəz edilsin. Maddə 2. Bu Qanun 2018 - ci il yanvarın 1 - dən qüvvəyə minir. İlham ƏLİYEV, Azərbaycan Respublikasının Prezidenti Bakı şəhəri, 1 dekabr 2017 - ci il № 913 - VQD', 'start_char_original': -1}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Hugging Face libraries\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline as hf_pipeline\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Scikit-learn for TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# NLTK for sentence tokenization (optional, but can be useful for chunking)\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Evaluation\n",
        "from evaluate import load as load_metric\n",
        "\n",
        "# Suppress Hugging Face informational messages\n",
        "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\" # error, warning, info, debug\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# --- 0. Setup & Data Loading/Preprocessing ---\n",
        "print(\"--- 0. Setup & Data Loading/Preprocessing ---\")\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading eqanun dataset...\")\n",
        "try:\n",
        "    dataset = load_dataset(\"allmalab/eqanun\", split=\"train\") # Take train split\n",
        "    # For demonstration, let's work with a subset\n",
        "    # dataset = dataset.select(range(1000)) # Using first 1000 documents\n",
        "    print(f\"Dataset loaded. Number of documents: {len(dataset)}\")\n",
        "    print(\"Dataset features:\", dataset.features)\n",
        "    print(\"Example document:\", dataset[0])\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Please ensure you have internet connectivity and the dataset name is correct.\")\n",
        "    exit()\n",
        "\n",
        "# Document Preprocessing and Chunking\n",
        "# Legal documents can be very long. We need to chunk them.\n",
        "# A simple strategy: split by paragraphs or fixed-size chunks.\n",
        "# Let's try fixed-size overlapping chunks for simplicity.\n",
        "\n",
        "MAX_CHUNK_LENGTH = 512  # Max tokens for model context (conservative)\n",
        "CHUNK_OVERLAP = 64      # Overlap between chunks\n",
        "\n",
        "def chunk_document(doc_id, text, tokenizer_for_counting, max_length=MAX_CHUNK_LENGTH, overlap=CHUNK_OVERLAP):\n",
        "    # Use a generic tokenizer just for counting tokens, not for actual model input yet\n",
        "    tokens = tokenizer_for_counting.encode(text, add_special_tokens=False)\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    doc_idx_counter = 0\n",
        "    while start < len(tokens):\n",
        "        end = min(start + max_length, len(tokens))\n",
        "        chunk_tokens = tokens[start:end]\n",
        "        chunk_text = tokenizer_for_counting.decode(chunk_tokens)\n",
        "        chunks.append({\n",
        "            \"doc_id\": f\"{doc_id}_{doc_idx_counter}\", # Unique ID for chunk\n",
        "            \"original_doc_id\": doc_id,\n",
        "            \"text\": chunk_text,\n",
        "            \"start_char_original\": text.find(chunk_text), # Approximate start char\n",
        "        })\n",
        "        doc_idx_counter += 1\n",
        "        if end == len(tokens):\n",
        "            break\n",
        "        start += (max_length - overlap)\n",
        "    return chunks\n",
        "\n",
        "# Use a basic tokenizer for chunking purposes (e.g., from SentenceTransformer)\n",
        "# or any BERT tokenizer. This is just for estimating token counts for splitting.\n",
        "# Using a multilingual model tokenizer is a good general choice.\n",
        "chunk_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "all_chunks = []\n",
        "print(\"Chunking documents...\")\n",
        "for i, doc in enumerate(tqdm(dataset)):\n",
        "    # Assuming 'text' field contains the main content.\n",
        "    # Check if 'text' is None or empty\n",
        "    if doc.get('text') and isinstance(doc['text'], str) and doc['text'].strip():\n",
        "        doc_chunks = chunk_document(doc.get('id', f\"doc_{i}\"), doc['text'], chunk_tokenizer)\n",
        "        all_chunks.extend(doc_chunks)\n",
        "    else:\n",
        "        print(f\"Warning: Document ID {doc.get('id', f'doc_{i}')} has empty or invalid text. Skipping.\")\n",
        "\n",
        "\n",
        "if not all_chunks:\n",
        "    print(\"No valid chunks were created. Exiting.\")\n",
        "    print(\"This might happen if the 'text' field in your dataset is consistently empty or not a string.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Total documents: {len(dataset)}. Total chunks created: {len(all_chunks)}\")\n",
        "if all_chunks:\n",
        "    print(\"Example chunk:\", all_chunks[0])\n",
        "\n",
        "# For easier access, let's put chunks into a list of texts and a list of metadata\n",
        "corpus_texts = [chunk['text'] for chunk in all_chunks]\n",
        "corpus_metadata = all_chunks # Keeps all info like doc_id, original_doc_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59c87a19",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c8a6b4432c0342639136f482ccef4e4c",
            "c17d481cea5c4f56947e5a9d30499d7e",
            "0b5b16fb62bd4f2a973bed566803c520",
            "52611df5a13442e6aaab40b052c17d9c",
            "24be58a104ae4f3aa79f883b953ac576",
            "1fdfa461d64c44ada236e02526a26fdf",
            "56e3f2dc643c42368d1f2ec9475a5f8c",
            "72f87ed71e99486d98df151da875a4cd",
            "358c17a0733b4fb890caad10ab92a50a",
            "d99cc81501f0435790b2d0b0871d8b94",
            "6afdf5aeff264070a12b0042ad9dd8c0",
            "106840e4758d4dbbb7abefc754520bdd"
          ]
        },
        "id": "59c87a19",
        "outputId": "32e4102d-4646-4d8c-be27-e54b912df3b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 1. Document Retrieval System ---\n",
            "\n",
            "--- 1.a Sparse Retrieval (TF-IDF) ---\n",
            "Fitting TF-IDF Vectorizer...\n",
            "TF-IDF matrix shape: (285300, 381068)\n",
            "\n",
            "--- 1.b Dense Retrieval (Sentence-BERT) ---\n",
            "Loading Sentence-BERT model: paraphrase-multilingual-mpnet-base-v2...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8a6b4432c0342639136f482ccef4e4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c17d481cea5c4f56947e5a9d30499d7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b5b16fb62bd4f2a973bed566803c520",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52611df5a13442e6aaab40b052c17d9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24be58a104ae4f3aa79f883b953ac576",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fdfa461d64c44ada236e02526a26fdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56e3f2dc643c42368d1f2ec9475a5f8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72f87ed71e99486d98df151da875a4cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "358c17a0733b4fb890caad10ab92a50a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d99cc81501f0435790b2d0b0871d8b94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6afdf5aeff264070a12b0042ad9dd8c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding corpus with Sentence-BERT (this might take a while)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "106840e4758d4dbbb7abefc754520bdd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/8916 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus embeddings shape: torch.Size([285300, 768])\n",
            "\n",
            "--- Testing Retrieval with query: 'Əmək müqaviləsinin ləğv edilməsi qaydaları hansılardır?' ---\n",
            "\n",
            "TF-IDF Retrieval Results:\n",
            "  Score: 0.2793 (ID: 46943_95)\n",
            "  Text: ##bsa, onda istər işçi, istərsə işəgötürən tərəfindən və istərsə də tərəflərin iradəsindən asılı olmayan hallarda bu Məcəllənin 68, 69, 70, 73, 74 və 75 - ci maddələrində nəzərdə tutulan əsaslara və q...\n",
            "\n",
            "  Score: 0.2672 (ID: 46943_45)\n",
            "  Text: sazişin tərəfləri ona müvafiq dəyişikliklər edə bilərlər. Maddə 41. Kollektiv sazişin yerinə yetirilməsinə nəzarət 1. Kollektiv sazişin yerinə yetirilməsinə nəzarəti tərəflər və müvafiq icra hakimiyyə...\n",
            "\n",
            "  Score: 0.2591 (ID: 16137_129)\n",
            "  Text: istifadə olunan qablar bioloji müayinədən və isitmə əməliyyatından keçirilirmi? Bəli ( ) Xeyr ( ) 11. 55. Hisə vermədə istifadə olunan materiallar hansılardır? Odun.......................................\n",
            "\n",
            "\n",
            "Sentence-BERT Retrieval Results:\n",
            "  Score: 0.8248 (ID: 54192_6)\n",
            "  Text: tərəfindən işə baxılması ona əsaslanır ki, əmək haqqının ödənilməsi mülkiyyətin əldə edilməsinin [UNK] qanuni gözlənti [UNK] sini təşkil edir. Bununla bağlı mübahisələndirilən ödəmə müqavilə və ya nor...\n",
            "\n",
            "  Score: 0.8241 (ID: 5409_46)\n",
            "  Text: müddəanın əvvəlcədən nəzərdə tutduğu müdafiə dairəsindən çıxara bilər. 2. Belə anlaşma mövcuddur ki, \" ödəmə qabiliyyəti olmayan \" termininin tərifini milli qanun və təcrübə verməlidir. 3. Həmin maddə...\n",
            "\n",
            "  Score: 0.8148 (ID: 46943_139)\n",
            "  Text: ##nin iki illik müddətinin cəmindən çox olmamalıdır. 2. İşəgötürənin təqsiri üzündən istehsalın, axın xəttinin və işin dayandırıldığı hallarda işçilərin qrup halında ödənişsiz məzuniyyətə buraxılması ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Document Retrieval System ---\n",
        "print(\"\\n--- 1. Document Retrieval System ---\")\n",
        "\n",
        "# --- 1.a Sparse Retrieval (TF-IDF) ---\n",
        "print(\"\\n--- 1.a Sparse Retrieval (TF-IDF) ---\")\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=None, token_pattern=r\"(?u)\\b\\w+\\b\") # Basic tokenizer\n",
        "print(\"Fitting TF-IDF Vectorizer...\")\n",
        "try:\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus_texts)\n",
        "    print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "\n",
        "    def retrieve_tfidf(query, top_k=5):\n",
        "        if tfidf_matrix.shape[0] == 0: # No documents indexed\n",
        "            return [], []\n",
        "        query_vector = tfidf_vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "        # Get top_k indices, sorted by similarity\n",
        "        # If similarities has very few non-zero values, argsort might not behave as expected for top_k\n",
        "        # Ensure we don't request more than available documents\n",
        "        actual_top_k = min(top_k, len(similarities))\n",
        "\n",
        "        # Efficiently get top_k indices:\n",
        "        if actual_top_k > 0:\n",
        "            # If you need to handle cases where fewer than top_k items have non-zero similarity:\n",
        "            # relevant_indices = np.argsort(similarities)[-actual_top_k:][::-1]\n",
        "            # A more robust way if many similarities are zero:\n",
        "            sorted_indices = np.argsort(similarities)[::-1] # Sort all in descending order\n",
        "            top_k_indices = sorted_indices[:actual_top_k]\n",
        "\n",
        "            # Filter out results with zero similarity if needed, though top_k usually handles this\n",
        "            # top_k_indices = [idx for idx in top_k_indices if similarities[idx] > 0]\n",
        "\n",
        "        else:\n",
        "            top_k_indices = []\n",
        "\n",
        "        retrieved_docs_data = [corpus_metadata[i] for i in top_k_indices]\n",
        "        retrieved_scores = [similarities[i] for i in top_k_indices]\n",
        "        return retrieved_docs_data, retrieved_scores\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"TF-IDF Error: {e}. This might happen if corpus_texts is empty or all documents are trivial.\")\n",
        "    tfidf_matrix = None # Ensure it's None so dependent functions can check\n",
        "    def retrieve_tfidf(query, top_k=5): # Dummy function\n",
        "        print(\"TF-IDF retriever not initialized due to previous error.\")\n",
        "        return [], []\n",
        "\n",
        "\n",
        "# --- 1.b Dense Retrieval (Sentence-BERT) ---\n",
        "print(\"\\n--- 1.b Dense Retrieval (Sentence-BERT) ---\")\n",
        "# Using a multilingual model is good for Azerbaijani\n",
        "# Other options: 'paraphrase-multilingual-MiniLM-L12-v2', 'distiluse-base-multilingual-cased-v1'\n",
        "sbert_model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
        "print(f\"Loading Sentence-BERT model: {sbert_model_name}...\")\n",
        "try:\n",
        "    sbert_model = SentenceTransformer(sbert_model_name)\n",
        "\n",
        "    # Encode the corpus (this can take time for large corpora)\n",
        "    print(\"Encoding corpus with Sentence-BERT (this might take a while)...\")\n",
        "    if corpus_texts: # only encode if there are texts\n",
        "        corpus_embeddings = sbert_model.encode(corpus_texts, convert_to_tensor=True, show_progress_bar=True)\n",
        "        print(f\"Corpus embeddings shape: {corpus_embeddings.shape}\")\n",
        "    else:\n",
        "        print(\"Corpus is empty. Skipping SBERT encoding.\")\n",
        "        corpus_embeddings = None #  torch.empty(0) if using torch tensors, or None\n",
        "\n",
        "\n",
        "    def retrieve_sbert(query, top_k=5):\n",
        "        if corpus_embeddings is None or corpus_embeddings.shape[0] == 0:\n",
        "            return [], []\n",
        "        query_embedding = sbert_model.encode(query, convert_to_tensor=True)\n",
        "        # Cosine similarity\n",
        "        cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "\n",
        "        # Ensuring we don't request more than available documents\n",
        "        actual_top_k = min(top_k, len(cos_scores))\n",
        "\n",
        "        # Get top_k results\n",
        "        if actual_top_k > 0:\n",
        "            top_results = torch.topk(cos_scores, k=actual_top_k)\n",
        "            top_k_indices = top_results.indices.tolist()\n",
        "            retrieved_scores = top_results.values.tolist()\n",
        "        else:\n",
        "            top_k_indices = []\n",
        "            retrieved_scores = []\n",
        "\n",
        "        retrieved_docs_data = [corpus_metadata[i] for i in top_k_indices]\n",
        "        return retrieved_docs_data, retrieved_scores\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing or using Sentence-BERT: {e}\")\n",
        "    sbert_model = None\n",
        "    corpus_embeddings = None\n",
        "    def retrieve_sbert(query, top_k=5): # Dummy function\n",
        "        print(\"Sentence-BERT retriever not initialized due to previous error.\")\n",
        "        return [], []\n",
        "\n",
        "\n",
        "# --- Test Retrieval ---\n",
        "sample_query_az = \"Əmək müqaviləsinin ləğv edilməsi qaydaları hansılardır?\" # \"What are the rules for termination of an employment contract?\"\n",
        "print(f\"\\n--- Testing Retrieval with query: '{sample_query_az}' ---\")\n",
        "\n",
        "print(\"\\nTF-IDF Retrieval Results:\")\n",
        "if tfidf_matrix is not None and tfidf_matrix.shape[0] > 0 :\n",
        "    tfidf_docs, tfidf_scores = retrieve_tfidf(sample_query_az, top_k=3)\n",
        "    for doc, score in zip(tfidf_docs, tfidf_scores):\n",
        "        print(f\"  Score: {score:.4f} (ID: {doc['doc_id']})\")\n",
        "        print(f\"  Text: {doc['text'][:200]}...\\n\")\n",
        "else:\n",
        "    print(\"  TF-IDF retrieval skipped due to earlier errors or empty corpus.\")\n",
        "\n",
        "print(\"\\nSentence-BERT Retrieval Results:\")\n",
        "if sbert_model and corpus_embeddings is not None and corpus_embeddings.shape[0] > 0:\n",
        "    sbert_docs, sbert_scores = retrieve_sbert(sample_query_az, top_k=3)\n",
        "    for doc, score in zip(sbert_docs, sbert_scores):\n",
        "        print(f\"  Score: {score:.4f} (ID: {doc['doc_id']})\")\n",
        "        print(f\"  Text: {doc['text'][:200]}...\\n\")\n",
        "else:\n",
        "    print(\"  Sentence-BERT retrieval skipped due to earlier errors or empty corpus.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2da5773a",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "bdf2bd85597741288ca0c2d9a79bfc22",
            "e178d9cdf0f84193805dd86d02ca67a1",
            "ed653bf674394ec8b014c8a25e169a9a",
            "6c0bcbd9e3e44789afb0b987542f6f72",
            "30ce0b4d0d474b18bafc64a6b5ba48b1",
            "2c3c9b132e3043cd944528dffcb90ba3",
            "140495f646af476daf98158e5ba3e8a4",
            "e6b3b3f1c5f04ad3bce4817206ef08e6",
            "55950e0a8e8d45f99c377eb9a688668f"
          ]
        },
        "id": "2da5773a",
        "outputId": "e20a16b1-578b-4cfd-c794-efb8f5cdac53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 2. RAG Pipeline ---\n",
            "Using Sentence-BERT for RAG pipeline retrieval.\n",
            "\n",
            "--- 2.a Answer Processing: Extractive Approach ---\n",
            "Loading Extractive QA model: deepset/xlm-roberta-base-squad2...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdf2bd85597741288ca0c2d9a79bfc22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e178d9cdf0f84193805dd86d02ca67a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed653bf674394ec8b014c8a25e169a9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c0bcbd9e3e44789afb0b987542f6f72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30ce0b4d0d474b18bafc64a6b5ba48b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Could not load extractive QA model: \n",
            " requires the protobuf library but it was not found in your environment. Checkout the instructions on the\n",
            "installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\n",
            "that match your environment. Please note that you may need to restart your runtime after installation.\n",
            "\n",
            "\n",
            "--- 2.b Answer Processing: Generative Approach ---\n",
            "Loading Generative QA model: google/mt5-small...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c3c9b132e3043cd944528dffcb90ba3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "140495f646af476daf98158e5ba3e8a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6b3b3f1c5f04ad3bce4817206ef08e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55950e0a8e8d45f99c377eb9a688668f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Could not load generative QA model: \n",
            " requires the protobuf library but it was not found in your environment. Checkout the instructions on the\n",
            "installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\n",
            "that match your environment. Please note that you may need to restart your runtime after installation.\n",
            "\n",
            "\n",
            "--- Testing RAG Pipeline with query: 'Əmək müqaviləsinin ləğv edilməsi qaydaları hansılardır?' ---\n",
            "\n",
            "Augmented Context (first 500 chars):\n",
            "tərəfindən işə baxılması ona əsaslanır ki, əmək haqqının ödənilməsi mülkiyyətin əldə edilməsinin [UNK] qanuni gözlənti [UNK] sini təşkil edir. Bununla bağlı mübahisələndirilən ödəmə müqavilə və ya normativ aktla müəyyən edilməli və işəgötürənin müvafiq öhdəlikləri yaranmış olmalıdır. Əmək Məcəlləsinin 172 - ci maddəsinin 5 - ci hissəsinə əsasən, əmək haqqının verilməsi işəgötürənin təqsiri üzündən gecikdirildikdə və bu hal fərdi əmək mübahisəsi yaratmayıbsa, hər gecikdirilmiş gün üçün işçiyə əmə...\n",
            "\n",
            "Retrieved 3 documents for RAG.\n",
            "Extractive Answer:\n",
            "  Answer: Extractive QA model not loaded.\n",
            "  Score: 0.0000\n",
            "\n",
            "Generative Answer:\n",
            "  Answer: Generative QA model not loaded.\n"
          ]
        }
      ],
      "source": [
        "# --- 2. Retrieval-Augmented Generation (RAG) Pipeline ---\n",
        "print(\"\\n--- 2. RAG Pipeline ---\")\n",
        "\n",
        "# Choose retriever for the RAG pipeline (e.g., SBERT if available, else TF-IDF)\n",
        "if sbert_model and corpus_embeddings is not None and corpus_embeddings.shape[0] > 0:\n",
        "    print(\"Using Sentence-BERT for RAG pipeline retrieval.\")\n",
        "    rag_retriever = retrieve_sbert\n",
        "elif tfidf_matrix is not None and tfidf_matrix.shape[0] > 0:\n",
        "    print(\"Sentence-BERT unavailable or corpus empty. Using TF-IDF for RAG pipeline retrieval.\")\n",
        "    rag_retriever = retrieve_tfidf\n",
        "else:\n",
        "    print(\"No retrievers available. RAG pipeline cannot function.\")\n",
        "    rag_retriever = None # Will cause pipeline to fail gracefully later\n",
        "\n",
        "def rag_pipeline_retrieve_and_augment(question, retriever_fn, top_k_retrieval=3):\n",
        "    if retriever_fn is None:\n",
        "        print(\"No retriever function available for RAG.\")\n",
        "        return \"\", []\n",
        "\n",
        "    # 1. Question Encoding (implicit in SBERT retriever, TF-IDF takes raw text)\n",
        "    # 2. Document Retrieval\n",
        "    retrieved_docs_data, _ = retriever_fn(question, top_k=top_k_retrieval)\n",
        "\n",
        "    if not retrieved_docs_data:\n",
        "        return \"No relevant documents found.\", []\n",
        "\n",
        "    # 3. Context Augmentation\n",
        "    # Concatenate texts of retrieved documents\n",
        "    # Be mindful of context window limits of the downstream QA model\n",
        "    context_parts = []\n",
        "    for doc_data in retrieved_docs_data:\n",
        "        context_parts.append(doc_data['text'])\n",
        "\n",
        "    # Simple concatenation. More sophisticated methods could be used.\n",
        "    augmented_context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    # Truncate context if too long for QA model (a general good practice)\n",
        "    # Most QA models have a limit around 512 tokens (context+question)\n",
        "    # This is a rough truncation based on characters; token-based is more accurate\n",
        "    # but requires the specific QA model's tokenizer.\n",
        "    # We will handle specific truncation within the QA model calls.\n",
        "\n",
        "    return augmented_context, retrieved_docs_data\n",
        "\n",
        "\n",
        "# --- 2.a Answer Processing: Extractive Approach ---\n",
        "print(\"\\n--- 2.a Answer Processing: Extractive Approach ---\")\n",
        "# Using a multilingual model fine-tuned on SQuAD-like tasks\n",
        "# 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
        "# 'deepset/xlm-roberta-large-squad2' or 'deepset/bert-base-multilingual-uncased-squad2'\n",
        "# For Azerbaijani, a multilingual model is preferred.\n",
        "extractive_qa_model_name = \"deepset/xlm-roberta-base-squad2\"\n",
        "print(f\"Loading Extractive QA model: {extractive_qa_model_name}...\")\n",
        "try:\n",
        "    extractive_qa_pipeline = hf_pipeline(\"question-answering\", model=extractive_qa_model_name, tokenizer=extractive_qa_model_name)\n",
        "except Exception as e:\n",
        "    print(f\"Could not load extractive QA model: {e}\")\n",
        "    extractive_qa_pipeline = None\n",
        "\n",
        "def answer_extractive(question, context):\n",
        "    if not extractive_qa_pipeline:\n",
        "        return {\"answer\": \"Extractive QA model not loaded.\", \"score\": 0}\n",
        "    if not context or context == \"No relevant documents found.\":\n",
        "        return {\"answer\": \"No context provided for extractive QA.\", \"score\": 0}\n",
        "\n",
        "    # The pipeline handles truncation if input is too long\n",
        "    # max_answer_len can be adjusted\n",
        "    try:\n",
        "        result = extractive_qa_pipeline(question=question, context=context, max_answer_len=100, truncation=True)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error during extractive QA: {e}\")\n",
        "        # Sometimes pipeline might fail if context is too short or unanswerable after truncation\n",
        "        return {\"answer\": f\"Error in extractive QA: {str(e)}\", \"score\": 0}\n",
        "\n",
        "\n",
        "# --- 2.b Answer Processing: Generative Approach ---\n",
        "print(\"\\n--- 2.b Answer Processing: Generative Approach ---\")\n",
        "# Using a multilingual T5 model like 'google/mt5-small'\n",
        "generative_model_name = \"google/mt5-small\"\n",
        "print(f\"Loading Generative QA model: {generative_model_name}...\")\n",
        "try:\n",
        "    generative_tokenizer = AutoTokenizer.from_pretrained(generative_model_name)\n",
        "    generative_model = AutoModelForSeq2SeqLM.from_pretrained(generative_model_name)\n",
        "except Exception as e:\n",
        "    print(f\"Could not load generative QA model: {e}\")\n",
        "    generative_tokenizer = None\n",
        "    generative_model = None\n",
        "\n",
        "def answer_generative(question, context, max_length=150):\n",
        "    if not generative_model or not generative_tokenizer:\n",
        "        return \"Generative QA model not loaded.\"\n",
        "    if not context or context == \"No relevant documents found.\":\n",
        "        return \"No context provided for generative QA.\"\n",
        "\n",
        "    # Format input for T5-style models\n",
        "    # mT5 was not specifically trained with \"question: ... context: ...\" prefix,\n",
        "    # but it's a common way to structure input for generation from context.\n",
        "    # For some models, just \"translate English to X: \" + question + \" context: \" + context might work\n",
        "    # Or simply prepending task prefix like \"cavab ver:\" (answer:)\n",
        "    input_text = f\"Sual: {question} Kontekst: {context} Cavab:\" # Azerbaijani: Question, Context, Answer\n",
        "\n",
        "    # Ensure input_text is not excessively long for the tokenizer\n",
        "    # Tokenize the input text\n",
        "    # T5 models typically have a 512 token limit for input. We need to truncate.\n",
        "    inputs = generative_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # Generate answer\n",
        "    # Adjust generation parameters as needed: num_beams, early_stopping, etc.\n",
        "    try:\n",
        "        outputs = generative_model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_length=max_length, # Max length of the generated answer\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        generated_answer = generative_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return generated_answer\n",
        "    except Exception as e:\n",
        "        print(f\"Error during generative QA: {e}\")\n",
        "        return f\"Error in generative QA: {str(e)}\"\n",
        "\n",
        "# --- Test RAG Pipeline ---\n",
        "print(f\"\\n--- Testing RAG Pipeline with query: '{sample_query_az}' ---\")\n",
        "\n",
        "if rag_retriever:\n",
        "    augmented_context, retrieved_docs_for_rag = rag_pipeline_retrieve_and_augment(sample_query_az, rag_retriever, top_k_retrieval=3)\n",
        "\n",
        "    print(f\"\\nAugmented Context (first 500 chars):\\n{augmented_context[:500]}...\\n\")\n",
        "    print(f\"Retrieved {len(retrieved_docs_for_rag)} documents for RAG.\")\n",
        "\n",
        "    print(\"Extractive Answer:\")\n",
        "    ext_answer = answer_extractive(sample_query_az, augmented_context)\n",
        "    print(f\"  Answer: {ext_answer.get('answer', 'N/A')}\")\n",
        "    print(f\"  Score: {ext_answer.get('score', 0):.4f}\")\n",
        "\n",
        "    print(\"\\nGenerative Answer:\")\n",
        "    gen_answer = answer_generative(sample_query_az, augmented_context)\n",
        "    print(f\"  Answer: {gen_answer}\")\n",
        "else:\n",
        "    print(\"RAG pipeline cannot be tested as no retriever is available.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afebc445",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7718e8db32744a9ab00c0b9e02e1c1da",
            "c38e9d5803904b48810018ef53c6f34a",
            "a2e7de36d29f41b1a190ddf037e185a1"
          ]
        },
        "id": "afebc445",
        "outputId": "b624bebc-dcbb-4eea-b7fe-590553d7885a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 3. Evaluation ---\n",
            "\n",
            "--- 3.a Evaluate Retrieval Component ---\n",
            "\n",
            "Evaluating TF-IDF Retriever:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7718e8db32744a9ab00c0b9e02e1c1da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating Retrieval:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  P@K: 0.0000\n",
            "  R@K: 0.0000\n",
            "  MRR: 0.0000\n",
            "\n",
            "Evaluating Sentence-BERT Retriever:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c38e9d5803904b48810018ef53c6f34a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating Retrieval:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  P@K: 0.4000\n",
            "  R@K: 1.0000\n",
            "  MRR: 1.0000\n",
            "\n",
            "--- 3.b Evaluate End-to-End Open-Domain QA ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2e7de36d29f41b1a190ddf037e185a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Skipping Extractive QA System evaluation (pipeline or retriever not initialized).\n",
            "\n",
            "Skipping Generative QA System evaluation (model, retriever, or ROUGE metric not initialized).\n",
            "\n",
            "--- End of Part 2: Open-Domain Question Answering System ---\n",
            "\n",
            "--- Example Query for Full RAG System ---\n",
            "Question: Boşanma üçün hansı sənədlər tələb olunur?\n",
            "\n",
            "Retrieved 3 documents. Context (first 200 chars): 12. Boşanma prosesində ər - arvadın ümumi əmlakının bölünməsi məsələsi də həll olunarsa, məhkəmə hər şeydən əvvəl hansı əmlakın bölünməli olmasını, bu əmlakın nə vaxt əldə edilməsini, ümumi mülkiyyətd...\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Evaluation ---\n",
        "print(\"\\n--- 3. Evaluation ---\")\n",
        "\n",
        "# For evaluation, we need a small set of (question, gold_retrieved_doc_ids, gold_answer_extractive, gold_answer_generative)\n",
        "# Since we don't have this for eqanun, we'll create a few mock examples\n",
        "# based on some hypothetical questions and answers we might derive by quickly looking at the data.\n",
        "# THIS IS A SIMPLIFIED DEMONSTRATION. Proper evaluation needs a curated dataset.\n",
        "\n",
        "# Let's assume we manually create some evaluation data.\n",
        "# For a real scenario, you'd need to carefully craft these.\n",
        "# I'll use the `sample_query_az` and *assume* some documents are relevant\n",
        "# and *manually* craft a hypothetical answer for demonstration.\n",
        "\n",
        "# Find some actual document IDs that might be relevant to sample_query_az from retrieval tests above\n",
        "# This is highly dependent on your actual data and retrieval results.\n",
        "# For example, if `retrieve_sbert` returned docs with ID 'doc_X_Y' and 'doc_A_B' as relevant:\n",
        "mock_relevant_doc_ids_for_query1 = []\n",
        "if 'sbert_docs' in locals() and sbert_docs: # Check if sbert_docs exists and is not empty\n",
        "    mock_relevant_doc_ids_for_query1 = [d['doc_id'] for d in sbert_docs[:2]] # Take top 2 SBERT results as \"gold\"\n",
        "elif 'tfidf_docs' in locals() and tfidf_docs:\n",
        "    mock_relevant_doc_ids_for_query1 = [d['doc_id'] for d in tfidf_docs[:2]] # Fallback to TF-IDF\n",
        "\n",
        "\n",
        "eval_data = [\n",
        "    {\n",
        "        \"question\": sample_query_az,\n",
        "        \"gold_retrieved_doc_ids\": mock_relevant_doc_ids_for_query1, # Placeholder\n",
        "        \"gold_answer_extractive\": \"Əmək müqaviləsinə xitam verilməsi üçün əsaslar Əmək Məcəlləsinin müvafiq maddələrində göstərilmişdir.\", # Placeholder\n",
        "        \"gold_answer_generative\": \"Əmək müqaviləsinin ləğvi, tərəflərin razılığı, müddətin bitməsi və ya qanunvericilikdə nəzərdə tutulmuş digər əsaslarla mümkündür.\", # Placeholder\n",
        "        # For generative answers, we'd list multiple good references for ROUGE\n",
        "        \"gold_references_generative\": [\n",
        "            \"Əmək müqaviləsinin ləğvi, tərəflərin razılığı, müddətin bitməsi və ya qanunvericilikdə nəzərdə tutulmuş digər əsaslarla mümkündür.\",\n",
        "            \"İşəgötürən və ya işçi tərəfindən əmək müqaviləsinə qanunda göstərilən qaydada xitam verilə bilər.\"\n",
        "        ]\n",
        "    },\n",
        "    # Add more questions here for a more robust evaluation\n",
        "]\n",
        "\n",
        "# --- 3.a Evaluate Retrieval Component ---\n",
        "print(\"\\n--- 3.a Evaluate Retrieval Component ---\")\n",
        "# Metrics: Precision@K, Recall@K, MRR. Requires knowing relevant documents for each query.\n",
        "\n",
        "def evaluate_retrieval(retriever_fn, eval_data, top_k_eval=5):\n",
        "    if not retriever_fn:\n",
        "        print(\"Retriever function not available for evaluation.\")\n",
        "        return {\"P@K\": 0, \"R@K\": 0, \"MRR\": 0}\n",
        "\n",
        "    precisions_at_k = []\n",
        "    recalls_at_k = []\n",
        "    reciprocal_ranks = []\n",
        "\n",
        "    for item in tqdm(eval_data, desc=\"Evaluating Retrieval\"):\n",
        "        query = item[\"question\"]\n",
        "        gold_doc_ids = set(item[\"gold_retrieved_doc_ids\"])\n",
        "\n",
        "        if not gold_doc_ids: # Skip if no gold documents are defined for this question\n",
        "            continue\n",
        "\n",
        "        retrieved_docs_data, _ = retriever_fn(query, top_k=top_k_eval)\n",
        "        retrieved_doc_ids = set([doc['doc_id'] for doc in retrieved_docs_data])\n",
        "\n",
        "        # Precision@K\n",
        "        num_relevant_retrieved = len(gold_doc_ids.intersection(retrieved_doc_ids))\n",
        "        precision_k = num_relevant_retrieved / len(retrieved_doc_ids) if retrieved_doc_ids else 0\n",
        "        precisions_at_k.append(precision_k)\n",
        "\n",
        "        # Recall@K\n",
        "        recall_k = num_relevant_retrieved / len(gold_doc_ids) if gold_doc_ids else 0\n",
        "        recalls_at_k.append(recall_k)\n",
        "\n",
        "        # MRR (Mean Reciprocal Rank)\n",
        "        rr = 0.0\n",
        "        for rank, doc_data in enumerate(retrieved_docs_data):\n",
        "            if doc_data['doc_id'] in gold_doc_ids:\n",
        "                rr = 1.0 / (rank + 1)\n",
        "                break\n",
        "        reciprocal_ranks.append(rr)\n",
        "\n",
        "    avg_precision_k = np.mean(precisions_at_k) if precisions_at_k else 0\n",
        "    avg_recall_k = np.mean(recalls_at_k) if recalls_at_k else 0\n",
        "    mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0\n",
        "\n",
        "    return {\"P@K\": avg_precision_k, \"R@K\": avg_recall_k, \"MRR\": mrr}\n",
        "\n",
        "# Evaluate TF-IDF Retriever\n",
        "if tfidf_matrix is not None and tfidf_matrix.shape[0] > 0 and any(d['gold_retrieved_doc_ids'] for d in eval_data):\n",
        "    print(\"\\nEvaluating TF-IDF Retriever:\")\n",
        "    tfidf_eval_results = evaluate_retrieval(retrieve_tfidf, eval_data, top_k_eval=5)\n",
        "    for metric, value in tfidf_eval_results.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "else:\n",
        "    print(\"\\nSkipping TF-IDF Retriever evaluation (no gold data or retriever not initialized).\")\n",
        "\n",
        "\n",
        "# Evaluate SBERT Retriever\n",
        "if sbert_model and corpus_embeddings is not None and corpus_embeddings.shape[0] > 0 and any(d['gold_retrieved_doc_ids'] for d in eval_data):\n",
        "    print(\"\\nEvaluating Sentence-BERT Retriever:\")\n",
        "    sbert_eval_results = evaluate_retrieval(retrieve_sbert, eval_data, top_k_eval=5)\n",
        "    for metric, value in sbert_eval_results.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "else:\n",
        "    print(\"\\nSkipping SBERT Retriever evaluation (no gold data or retriever not initialized).\")\n",
        "\n",
        "\n",
        "# --- 3.b Evaluate End-to-End Open-Domain QA ---\n",
        "print(\"\\n--- 3.b Evaluate End-to-End Open-Domain QA ---\")\n",
        "# Metrics: Exact Match (EM), F1-score for extractive. ROUGE for generative.\n",
        "\n",
        "# Helper for EM/F1 (from SQuAD evaluation script, simplified)\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    import string, re\n",
        "    def remove_articles(text): # Azerbaijani doesn't have articles like 'a', 'an', 'the'\n",
        "        return text\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = len(set(prediction_tokens) & set(ground_truth_tokens))\n",
        "    if common == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * common / len(prediction_tokens)\n",
        "    recall = 1.0 * common / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "# ROUGE metric loader\n",
        "try:\n",
        "    rouge_metric = load_metric(\"rouge\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load ROUGE metric: {e}. Generative evaluation will be skipped.\")\n",
        "    rouge_metric = None\n",
        "\n",
        "# Evaluate Extractive QA\n",
        "if extractive_qa_pipeline and rag_retriever:\n",
        "    print(\"\\nEvaluating Extractive QA System:\")\n",
        "    extractive_ems = []\n",
        "    extractive_f1s = []\n",
        "    for item in tqdm(eval_data, desc=\"Evaluating Extractive QA\"):\n",
        "        question = item[\"question\"]\n",
        "        gold_answer = item[\"gold_answer_extractive\"]\n",
        "\n",
        "        context, _ = rag_pipeline_retrieve_and_augment(question, rag_retriever)\n",
        "        if context == \"No relevant documents found.\" or not context.strip():\n",
        "            predicted_answer_obj = {\"answer\": \"\"} # Handle no context\n",
        "        else:\n",
        "            predicted_answer_obj = answer_extractive(question, context)\n",
        "\n",
        "        predicted_answer = predicted_answer_obj.get('answer', \"\")\n",
        "\n",
        "        extractive_ems.append(exact_match_score(predicted_answer, gold_answer))\n",
        "        extractive_f1s.append(f1_score(predicted_answer, gold_answer))\n",
        "\n",
        "    avg_em = np.mean(extractive_ems) if extractive_ems else 0\n",
        "    avg_f1 = np.mean(extractive_f1s) if extractive_f1s else 0\n",
        "    print(f\"  Exact Match (EM): {avg_em:.4f}\")\n",
        "    print(f\"  F1 Score: {avg_f1:.4f}\")\n",
        "else:\n",
        "    print(\"\\nSkipping Extractive QA System evaluation (pipeline or retriever not initialized).\")\n",
        "\n",
        "\n",
        "# Evaluate Generative QA\n",
        "if generative_model and generative_tokenizer and rag_retriever and rouge_metric:\n",
        "    print(\"\\nEvaluating Generative QA System:\")\n",
        "    all_predictions_gen = []\n",
        "    all_references_gen = [] # ROUGE expects list of lists of references\n",
        "\n",
        "    for item in tqdm(eval_data, desc=\"Evaluating Generative QA\"):\n",
        "        question = item[\"question\"]\n",
        "        # ROUGE can take multiple reference answers\n",
        "        gold_references = item.get(\"gold_references_generative\", [item[\"gold_answer_generative\"]])\n",
        "\n",
        "        context, _ = rag_pipeline_retrieve_and_augment(question, rag_retriever)\n",
        "        if context == \"No relevant documents found.\" or not context.strip():\n",
        "            predicted_answer = \"\" # Handle no context\n",
        "        else:\n",
        "            predicted_answer = answer_generative(question, context)\n",
        "\n",
        "        all_predictions_gen.append(predicted_answer)\n",
        "        all_references_gen.append(gold_references) # Store list of references\n",
        "\n",
        "    if all_predictions_gen and all_references_gen:\n",
        "        rouge_results = rouge_metric.compute(predictions=all_predictions_gen, references=all_references_gen)\n",
        "        print(f\"  ROUGE Scores: \")\n",
        "        for key, value in rouge_results.items():\n",
        "            print(f\"    {key}: {value:.4f}\")\n",
        "    else:\n",
        "        print(\"  Not enough data to compute ROUGE scores.\")\n",
        "else:\n",
        "    print(\"\\nSkipping Generative QA System evaluation (model, retriever, or ROUGE metric not initialized).\")\n",
        "\n",
        "\n",
        "print(\"\\n--- End of Part 2: Open-Domain Question Answering System ---\")\n",
        "\n",
        "# Example of using the full RAG system with a new question\n",
        "if rag_retriever:\n",
        "    print(\"\\n--- Example Query for Full RAG System ---\")\n",
        "    new_question_az = \"Boşanma üçün hansı sənədlər tələb olunur?\" # \"What documents are required for divorce?\"\n",
        "\n",
        "    print(f\"Question: {new_question_az}\")\n",
        "\n",
        "    # 1. Retrieve and Augment\n",
        "    context_for_new_q, retrieved_docs_new_q = rag_pipeline_retrieve_and_augment(new_question_az, rag_retriever, top_k_retrieval=3)\n",
        "    print(f\"\\nRetrieved {len(retrieved_docs_new_q)} documents. Context (first 200 chars): {context_for_new_q[:200]}...\")\n",
        "\n",
        "    # 2. Extractive Answer\n",
        "    if extractive_qa_pipeline:\n",
        "        ext_ans_new_q = answer_extractive(new_question_az, context_for_new_q)\n",
        "        print(f\"\\nExtractive Answer: {ext_ans_new_q.get('answer', 'N/A')} (Score: {ext_ans_new_q.get('score',0):.2f})\")\n",
        "\n",
        "    # 3. Generative Answer\n",
        "    if generative_model:\n",
        "        gen_ans_new_q = answer_generative(new_question_az, context_for_new_q)\n",
        "        print(f\"\\nGenerative Answer: {gen_ans_new_q}\")\n",
        "else:\n",
        "    print(\"\\nSkipping example query as RAG retriever is not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a54dc09",
      "metadata": {
        "id": "9a54dc09"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
